https://www.youtube.com/watch?v=6BD-Vv-ViBw // Hive Bucketing in Apache Spark - Tejas Patil
https://stackoverflow.com/questions/22141631/what-is-the-purpose-of-shuffling-and-sorting-phase-in-the-reducer-in-map-reduce

types of join ::
Broadcaste hash join
shuffle hash join
sort merge join
----------------------------------------bucketing of table using spark--------------------------------------------- 
lounch with spark 2.2
1} using dataframe --df.write.bucketBy(numbuckets,"col1","col2" .....).sortBy("col1", "col2").saveAsTable("buckettable")
2} using sql statement - create table buckettable("col1","col2"..) using parquet clustered by (col1) sorted by(col1) into n buckets;

Check bucketing spec::
pyspark> sqlContext.sql("desc formatted bucktable").collect().foreach(println)
bucket configuration ::
SET spark.sql.sources.bucketing.enabled = true;

differnce btw hive bucketing and spark bucketing ::
hive bucket ::there will be one file for one bucket. means it follows map reduce concept.optimized write and write are costely.Each bucket is sorted globally.this make easy to optimize queries reading data.
spark bucket:: there will be n number of file for one bucket , so it is not globallly sorted. each file is invidually sorted but "buckets" are not globally sorted.
----------------------------------------SQL Planner improvement --------------------------------------
Suffle and sort is costly due to disk and network IO.
Bucketing will pre -(shuffle and sort) the inputs.
Expect at least 2-5x gains after bucketing input tables for joins.
Spark bucketing is not compatible with hive bucketing.

