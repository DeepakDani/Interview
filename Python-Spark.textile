what is sqlcontext.sql ?
sql. SQLContext(sc) // this is used to implicitly convert an RDD to a DataFrame. ... SQLContext sqlContext = new org.apache.spark.sql.SQLContext(sc); The entry point into all relational functionality in Spark is the SQLContext class, or one of its decedents.
----------------------------------------------------------------------
https://dzone.com/articles/best-spark-sql-example-in-10-steps
-----------------------------------------------------------------------
RDD is an immutable distributed collection of data, partitioned across nodes in the cluster that can be operated in parallel with a low-level API that offers transformations and actions.
limitations of RDD -
1) No input optimization engine---
There is no provision in RDD for automatic optimization.
2 )there is no Static typing and run-time type safety in RDD. It does not allow us to check error at the runtime.

3)Degrade when not enough memory
The RDD degrades when there is not enough memory to store RDD in-memory or on disk. There comes storage issue when there is a lack of memory to store RDD. The partitions that overflow from RAM can be stored on disk and will provide the same level of performance. By increasing the size of RAM and disk it is possible to overcome this issue.

4) Handling structured data
RDD does not provide schema view of data. It has no provision for handling structured data.

Dataset and DataFrame provide the Schema view of data. It is a distributed collection of data organized into named columns.
----------------------------------------------------------------------------------------------
Compare Spark vs Hadoop MapReduce
Criteria  Hadoop MapReduce/Apache Spark
Disk usage  MapReduce is disk oriented.//Spark caches data in-memory and ensures low latency.
Processing Only batch processing is supported//Supports real-time processing through spark streaming.
Installation  is bound to hadoop. //Is not bound to Hadoop.
-----------------------------------------------------------------------------------------------------------
how many modes are there for spark execution??
1.local
2.standalone
3.yarn(default)
4.mesos
syntax: pyspark --master local

SparkContext.wholeTextFiles lets you read a directory containing multiple small text files, and returns each of them as (filename, content) pairs. This is in contrast with textFile, which would return one record per line in each file.

RDD.saveAsPickleFile and SparkContext.pickleFile support saving an RDD in a simple format consisting of pickled Python objects. Batching is used on pickle serialization, with default batch size 10.

RDDs support two types of operations: transformations, which create a new dataset from an existing one, and actions, which return a value to the driver program after running a computation on the dataset.
-----------------------------------------------------------------------------------------------------------
http://www.secnetix.de/olli/Python/lambda_functions.hawk

print reduce(lambda x,y:x+y, map(lambda w : w[1],[(1,2),(3,4),(5,6)])) 
reduce (lambda x,y: x+y, map(lambda x :x[1], filter(lambda x : x[0]%2==1,[(1,2),(3,4),(5,6)])))

----------------------------
for loading hive table 
from pyspark import HiveContext
sqlContext = HiveContext(sc)
data = sqlContext.sql("select * from table")
for i in data.collect():
   print(i)///// for all values
    print(i.department_id)// for selected columns 
--------------------------
for loading JSON file 
from pyspark import SQLContext
sqlConext = SQLConext(sc)
departmewtnjson= sqlConext.jsonFile("")
departmewtnjson.registerTempTable("jsontable")
sqlContext.sql("select * from dson")
for i in sqlContext.sql("select * from dson") :
  print(i)
------------------------------------------------
#calculate aggregate statistics
orderRdd = sc.textFile("")
orderMap = orderRdd.map(lambda x : split(",")[1],1))
ordebystatus = orderMap.groupByKey(lambda x,y : x+y)
orderBystatus = orderMap.reduceByKey(lambda x,y : x+y)

difference btw reduceByKey and groupByKey
https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/best_practices/prefer_reducebykey_over_groupbykey.html
Notice how pairs on the same machine with the same key are combined (by using the lamdba function passed into reduceByKey) before the data is shuffled. Then the lamdba function is called again to reduce all the values from each partition to produce one final result.
---------------------------------------------------- 
word count program --
ratnesh1 = sc.teextFile(" ")
ratnesh1.map(lambda x: (x[0],1)).reduceBYKey(lambda x,y: x + y).collect()
---------------------------------------------------------
joining two datasets--
ordersRDD = sc.textFile(" ")
orderMapRdd = ordersRDD.map(lambda o : (int(o.split(",")[0]),o.split(",")[1])) /// for two record from file
//////orderMapRdd = ordersRDD.map(lambda o : (int(o.split(",")[0]),o)) ///  for all records 
orderjoinitem = orderMapRdd.join(ordersRDD)
joindata = orderjoinitem.map(lambda t:t[1][0].split(",")[1],float(t[1][1].split(",")[4])))/// for getting special records
ordeperday = joindata.map(lambda t: (t[1][1].split(",")[1] + "," +str[t[0]))).distinct()  //distinct for unique key,

By Default HiveContext/sqlContext using 200 tasks manager
but we can set 
from pyspark.sql import HiveContext
sqlContext = HiveContext(sc)
sqlContext.sql("set spark.sql.shuffle.partitions=10")
--------------------------------------------------------------
# using spark native sql
from pyspark.sql import SQLContext, Row
sqlContext = SQLConext(sc)
sqlContext.sql(set spark.sql.shuffle.partitions=10");

orderrdd = sq.textFile("")
orderMap = orderrdd.map(lambda x : x.split(",")).map(lambda x : Row(id=x[0],first_name= x[1],gender = x[2]))
orderschema = sqlContext.inferSchema(orders)
orderSchema.registerTempTable("orders")
Spark supports two types of shared variables: broadcast variables, which can be used to cache a value in memory on all nodes, and accumulators, which are variables that are only “added” to, such as counters and sums.

---------------------------------------------------------------------------------------------------------
spark document importent points--
The first thing a Spark program must do is to create a SparkContext object, which tells Spark how to access a cluster. To create a SparkContext you first need to build a SparkConf object that contains information about your application.

conf = SparkConf().setAppName(appName).setMaster(master)
sc = SparkContext(conf=conf)

Apart from text files, Spark’s Python API also supports several other data formats:

SparkContext.wholeTextFiles lets you read a directory containing multiple small text files, and returns each of them as (filename, content) pairs. This is in contrast with textFile, which would return one record per line in each file.

RDD.saveAsPickleFile and SparkContext.pickleFile support saving an RDD in a simple format consisting of pickled Python objects. Batching is used on pickle serialization, with default batch size 10.
----------------------------------------------------------------------------
Passing Functions to Spark
Spark’s API relies heavily on passing functions in the driver program to run on the cluster. There are three recommended ways to do this:

Lambda expressions, for simple functions that can be written as an expression. (Lambdas do not support multi-statement functions or statements that do not return a value.)
Local defs inside the function calling into Spark, for longer code.
------------------------------------------------------------------------------
groupBy() & groupByKey() Example---- groupByKey() operates on Pair RDDs and is used to group all the values related to a given key. groupBy() can be used in both unpaired & paired RDDs. When used with unpaired data, the key for groupBy() is decided by the function literal passed to the method


--------------------------------------------------------------------------------
What's the difference between an RDD's map and mapPartitions method?
The method map converts each element of the source RDD into a single element of the result RDD by applying a function. mapPartitions converts each partition of the source RDD into multiple elements of the result (possibly none).

And does flatMap behave like map or like mapPartitions?
Neither, flatMap works on a single element (as map) and produces multiple elements of the result (as mapPartitions).
------------------------------------------------------------------------------
Transformation returns RDD but action returns value or save in any file format.
Transformations are lazy the result RDD is not immediate computed.
Actions are eager their result is immediate computed.



