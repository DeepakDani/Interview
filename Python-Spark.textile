what is sqlcontext.sql ?
sql. SQLContext(sc) // this is used to implicitly convert an RDD to a DataFrame. ... SQLContext sqlContext = new org.apache.spark.sql.SQLContext(sc); The entry point into all relational functionality in Spark is the SQLContext class, or one of its decedents.
----------------------------------------------------------------------
https://dzone.com/articles/best-spark-sql-example-in-10-steps
-----------------------------------------------------------------------
RDD is an immutable distributed collection of data, partitioned across nodes in the cluster that can be operated in parallel with a low-level API that offers transformations and actions.
limitations of RDD -
1) No input optimization engine---
There is no provision in RDD for automatic optimization.
2 )there is no Static typing and run-time type safety in RDD. It does not allow us to check error at the runtime.

3)Degrade when not enough memory
The RDD degrades when there is not enough memory to store RDD in-memory or on disk. There comes storage issue when there is a lack of memory to store RDD. The partitions that overflow from RAM can be stored on disk and will provide the same level of performance. By increasing the size of RAM and disk it is possible to overcome this issue.

4) Handling structured data
RDD does not provide schema view of data. It has no provision for handling structured data.

Dataset and DataFrame provide the Schema view of data. It is a distributed collection of data organized into named columns.
----------------------------------------------------------------------------------------------
Compare Spark vs Hadoop MapReduce
Criteria  Hadoop MapReduce/Apache Spark
Disk usage  MapReduce is disk oriented.//Spark caches data in-memory and ensures low latency.
Processing Only batch processing is supported//Supports real-time processing through spark streaming.
Installation  is bound to hadoop. //Is not bound to Hadoop.
-----------------------------------------------------------------------------------------------------------
how many modes are there for spark execution??
1.local
2.standalone
3.yarn(default)
4.mesos
syntax: pyspark --master local

SparkContext.wholeTextFiles lets you read a directory containing multiple small text files, and returns each of them as (filename, content) pairs. This is in contrast with textFile, which would return one record per line in each file.

RDD.saveAsPickleFile and SparkContext.pickleFile support saving an RDD in a simple format consisting of pickled Python objects. Batching is used on pickle serialization, with default batch size 10.

RDDs support two types of operations: transformations, which create a new dataset from an existing one, and actions, which return a value to the driver program after running a computation on the dataset.
-----------------------------------------------------------------------------------------------------------
http://www.secnetix.de/olli/Python/lambda_functions.hawk

print reduce(lambda x,y:x+y, map(lambda w : w[1],[(1,2),(3,4),(5,6)])) 
reduce (lambda x,y: x+y, map(lambda x :x[1], filter(lambda x : x[0]%2==1,[(1,2),(3,4),(5,6)])))

----------------------------
for loading hive table 
from pyspark import HiveContext
sqlContext = HiveContext(sc)
data = sqlContext.sql("select * from table")
for i in data.collect():
   print(i)///// for all values
    print(i.department_id)// for selected columns 
--------------------------
for loading JSON file 
from pyspark import SQLContext
sqlConext = SQLConext(sc)
departmewtnjson= sqlConext.jsonFile("")
departmewtnjson.registerTempTable("jsontable")
sqlContext.sql("select * from dson")
for i in sqlContext.sql("select * from dson") :
  print(i)
------------------------------------------------
#calculate aggregate statistics
orderRdd = sc.textFile("")
orderMap = orderRdd.map(lambda x : split(",")[1],1))
ordebystatus = orderMap.groupByKey(lambda x,y : x+y)
orderBystatus = orderMap.reduceByKey(lambda x,y : x+y)

difference btw reduceByKey and groupByKey
https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/best_practices/prefer_reducebykey_over_groupbykey.html
Notice how pairs on the same machine with the same key are combined (by using the lamdba function passed into reduceByKey) before the data is shuffled. Then the lamdba function is called again to reduce all the values from each partition to produce one final result.
---------------------------------------------------- 
word count program --
ratnesh1 = sc.teextFile(" ")
ratnesh1.map(lambda x: (x[0],1)).reduceBYKey(lambda x,y: x + y).collect()
---------------------------------------------------------
joining two datasets--
ordersRDD = sc.textFile(" ")
orderMapRdd = ordersRDD.map(lambda o : (int(o.split(",")[0]),o.split(",")[1])) /// for two record from file
//////orderMapRdd = ordersRDD.map(lambda o : (int(o.split(",")[0]),o)) ///  for all records 
for i in orderMapRdd.take(5):
   print(i)







